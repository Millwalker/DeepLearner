{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f819a69e-72a2-4f06-8d14-ee7fddde06e8",
   "metadata": {},
   "source": [
    "<center><h1>Challenge RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb3b385-0c22-44fe-ab9c-c828b0fe697c",
   "metadata": {},
   "source": [
    "In this challenge, the gaol is to train a classifier for sequences of genetic code.\n",
    "\n",
    "Each sequence is represented by a string of letters [‘A’, ‘C’, ‘G’, ’T’] and belongs to one of five categories/classes labelled [0,…,4].\n",
    "\n",
    "For training purposes, you will find 400 labelled sequences, each of length 400 characters (sequences: **data_x**, labels: **data_y**).\n",
    "\n",
    "To validate your model, you have a further 100 labelled sequences (**val_x**, **val_y**) with 1200 characters each.\n",
    "\n",
    "Finally, you have 250 unlabeled sequences (**test_x**, 2000 characters) which need to be classified.\n",
    "\n",
    "Hint: Training recurrent networks is very expensive! Do not start working on this challenge too late or you might not manage to finish in time.\n",
    "\n",
    "Your task is to train an RNN-based classifier and make a prediction for the missing labels of the test set (**test_x** in the attached archive). Store your prediction as a one-dimensional **numpy.ndarray**, save this array as **prediction.npy**, and upload this file to the whiteboard.\n",
    "\n",
    "To load  the data and save your prediction, please refer to the following code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "474982fa-abc6-4b0c-9446-69fa78600c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400,) <U400\n",
      "(400,) int64\n",
      "(100,) <U1200\n",
      "(100,) int64\n",
      "(250,) <U2000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open('rnn-challenge-data.npz', 'rb') as f:\n",
    "    X = np.load(f)\n",
    "    data_x = X['data_x']\n",
    "    data_y = X['data_y']\n",
    "    val_x = X['val_x']\n",
    "    val_y = X['val_y']\n",
    "    test_x = X['test_x']\n",
    "\n",
    "# TRAINING DATA: INPUT (x) AND OUTPUT (y)\n",
    "print(data_x.shape, data_x.dtype)\n",
    "print(data_y.shape, data_y.dtype)\n",
    "\n",
    "# VALIDATION DATA: INPUT (x) AND OUTPUT (y)\n",
    "print(val_x.shape, val_x.dtype)\n",
    "print(val_y.shape, val_y.dtype)\n",
    "\n",
    "# TEST DATA: INPUT (x) ONLY\n",
    "print(test_x.shape, test_x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21c1adeb-1a35-4b26-81d0-963c91be821d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTAGCTGAGCTACTGAGCTACAGTTGACTGACCAGTCAGTGCTAGCTACTGACAGTCTGACAGTTGACCTGACTGATGACCAGTCTAGCAGTGCTACTAGCTAGGCTACAGTCAGTTGACCAGTCTGACAGTCAGTCTGACTGACAGTCAGTCTAGGCTATGACCTGACTGATGACCTGACTGACTGACAGTCTGACTGATGACGCTATGACCTGACTAGCTAGCAGTTGACTGACCTGACAGTGCTACTAGCAGTTGACCAGTGCTACAGTCTGATGACTGACCTGACAGTCTAGGCTACAGTTGACCTGACAGTCAGTGCTACTGACAGTCTAGTGACCAGTCAGTCAGTTGACCTGACTAGCAGTTGACGCTATGACCAGTCTGACAGTGCTACTAG\n"
     ]
    }
   ],
   "source": [
    "# Printing one for having an idea\n",
    "print(data_x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6cb7d5-25c9-46cd-872e-04b33d98025f",
   "metadata": {},
   "source": [
    "# Code\n",
    "We could try\n",
    "* Simple RNN\n",
    "* LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "509aff0c-e2b2-4d89-a412-357684ff0b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e850389-4d8c-4161-bb26-bd82d9795df0",
   "metadata": {},
   "source": [
    "<center>Here making us of this tutorial : [NLP From Scratch: Classifying Names with a Character level RNN](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html) (from Pytorch)</center>\n",
    "\n",
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94b13b18-8d09-4864-b5dc-73831d6fa6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_categories = 5              # [0, 1, 2, 3, 4]\n",
    "\n",
    "all_letters = 'ACGT' # string.ascii\n",
    "n_letters = len(all_letters)\n",
    "# Note that the two last lines could be maybe simpler with n_letters = 4 and giving just the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da36cca8-7fa4-448a-a3a9-20441bd470af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0.]])\n",
      "torch.Size([8, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Testing that everything works\n",
    "print(letterToTensor('A'))\n",
    "print(lineToTensor('ACGTTAGC').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7898b9ac-d574-4964-9d8e-b36723c81aa2",
   "metadata": {},
   "source": [
    "Turning all the data to tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ff606a4-a9ad-4a6c-89eb-d75529e25c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x_torch = torch.stack([lineToTensor(data_x[i]) for i in range(len(data_x))])\n",
    "val_x_torch = torch.stack([lineToTensor(val_x[i]) for i in range(len(val_x))])\n",
    "test_x_torch = torch.stack([lineToTensor(test_x[i]) for i in range(len(test_x))])\n",
    "\n",
    "data_y_torch = torch.from_numpy(data_y)\n",
    "val_y_torch = torch.from_numpy(val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b642e58-3135-4100-afb7-01090cfcf3ed",
   "metadata": {},
   "source": [
    "## Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14a27ca6-f80b-4aed-961c-96cdd220e146",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "n_hidden = 128\n",
    "rnn = RNN(n_letters, n_hidden, n_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588c37ed-470a-411c-8065-7539f14e0781",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6f4fab6-4f99-4e25-b627-1e651fe7d93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.6278, -1.6485, -1.5223, -1.6879, -1.5694]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Initializing the network, trying with a single letter\n",
    "input = letterToTensor('A')\n",
    "hidden = torch.zeros(1, n_hidden)\n",
    "\n",
    "output, next_hidden = rnn(input, hidden)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25b5b26f-0158-41a6-b43b-0e3afbf2ec67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5989, -1.7224, -1.5921, -1.5640, -1.5778]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Trying with a sequence\n",
    "input = data_x_torch[1]\n",
    "hidden = torch.zeros(1, n_hidden)\n",
    "\n",
    "output, next_hidden = rnn(input[0], hidden)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b98d003-d63e-4a15-bea8-0c69ebf0987d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# Helper function for classification\n",
    "def categoryFromOutput(output):\n",
    "    _, top_i = output.topk(1)\n",
    "    return top_i[0].item()\n",
    "\n",
    "print(categoryFromOutput(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c57886d-2e6a-4594-a37d-95ea3d65fd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "378fddb9-db3f-4a3e-8a4f-39cb1c1300c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(category_tensor, line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "    \n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "\n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f409b6a-1468-458c-b338-0a2c8485a373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9be3736587bc4cca810b40b7daa6b05a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[-1.6144, -1.6876, -1.5271, -1.5817, -1.6438]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5213, -1.8371, -1.6051, -1.5436, -1.5710]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5639, -1.7723, -1.4944, -1.7096, -1.5351]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5271, -1.8436, -1.5624, -1.5757, -1.5698]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5280, -1.8222, -1.4939, -1.7371, -1.5103]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5342, -1.7818, -1.5175, -1.6976, -1.5434]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5372, -1.7866, -1.5232, -1.6778, -1.5479]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5473, -1.9077, -1.5796, -1.5539, -1.5085]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5314, -1.9159, -1.5788, -1.5547, -1.5183]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5555, -1.7992, -1.5111, -1.6798, -1.5306]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5935, -1.7213, -1.5283, -1.5502, -1.6668]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5312, -1.8620, -1.5838, -1.5459, -1.5605]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6094, -1.9002, -1.5025, -1.5745, -1.5106]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5366, -1.8720, -1.5677, -1.5420, -1.5673]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6075, -1.7269, -1.5296, -1.5419, -1.6546]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5421, -1.8792, -1.5645, -1.5270, -1.5750]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5555, -1.9382, -1.5653, -1.5447, -1.5026]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5221, -1.8874, -1.6141, -1.4998, -1.5700]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5446, -1.8862, -1.5690, -1.5328, -1.5569]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5538, -1.9441, -1.5633, -1.5622, -1.4858]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5458, -1.8338, -1.4923, -1.6584, -1.5524]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5385, -1.9518, -1.5567, -1.5694, -1.4949]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5296, -1.8740, -1.4782, -1.6866, -1.5293]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5366, -1.8815, -1.4763, -1.6643, -1.5384]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5338, -1.8860, -1.5565, -1.5335, -1.5801]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5901, -1.8840, -1.5192, -1.5338, -1.5630]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5417, -1.8271, -1.4928, -1.6725, -1.5485]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5261, -1.8948, -1.5579, -1.5437, -1.5697]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.4993, -1.9079, -1.5838, -1.5214, -1.5862]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5466, -1.8376, -1.4620, -1.6877, -1.5554]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5589, -1.9039, -1.5039, -1.5454, -1.5840]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5050, -1.9150, -1.5378, -1.5443, -1.5978]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5168, -1.8870, -1.5394, -1.5616, -1.5859]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5232, -1.8893, -1.5473, -1.5657, -1.5652]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5820, -1.7521, -1.4938, -1.5643, -1.6752]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5030, -1.8976, -1.5575, -1.5615, -1.5745]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5651, -1.7618, -1.5078, -1.5399, -1.6962]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5818, -1.7387, -1.5055, -1.5536, -1.6859]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5118, -1.8449, -1.4813, -1.6994, -1.5544]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5191, -1.8525, -1.4835, -1.6739, -1.5612]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5252, -1.8549, -1.4894, -1.6788, -1.5425]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5707, -1.9179, -1.5095, -1.5887, -1.5142]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5009, -1.8912, -1.5742, -1.5298, -1.5974]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5057, -1.8942, -1.5569, -1.5348, -1.6023]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5356, -1.8645, -1.4927, -1.6430, -1.5534]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5317, -1.8700, -1.5007, -1.6378, -1.5497]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5266, -1.8827, -1.5616, -1.5383, -1.5798]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5747, -1.8797, -1.5185, -1.5240, -1.5925]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5803, -1.8804, -1.5024, -1.5355, -1.5914]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.4938, -1.8925, -1.5672, -1.5440, -1.5964]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5052, -1.9003, -1.5435, -1.5456, -1.6010]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5070, -1.9001, -1.5549, -1.5529, -1.5797]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5312, -1.8558, -1.4874, -1.6732, -1.5427]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5328, -1.8281, -1.4989, -1.6588, -1.5625]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5398, -1.8666, -1.4951, -1.6673, -1.5235]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.4820, -1.8987, -1.5915, -1.5424, -1.5820]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5922, -1.7250, -1.5167, -1.5459, -1.6832]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5086, -1.9001, -1.5392, -1.5739, -1.5728]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5385, -1.8205, -1.5026, -1.6602, -1.5573]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6080, -1.7315, -1.4930, -1.5558, -1.6768]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5139, -1.9118, -1.5235, -1.5506, -1.5992]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5763, -1.8799, -1.4848, -1.5576, -1.5924]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5442, -1.8181, -1.4732, -1.6713, -1.5751]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5516, -1.8491, -1.4796, -1.6958, -1.5162]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5610, -1.8562, -1.4790, -1.6971, -1.5018]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5044, -1.9565, -1.5584, -1.5947, -1.5004]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5503, -1.8952, -1.5019, -1.5713, -1.5748]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5384, -1.9021, -1.5020, -1.5774, -1.5759]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5589, -1.9433, -1.4842, -1.6206, -1.5055]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5608, -1.9467, -1.4888, -1.6018, -1.5136]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5491, -1.9119, -1.4947, -1.5537, -1.5898]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5479, -1.8430, -1.4788, -1.6751, -1.5429]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5535, -1.8447, -1.4653, -1.6785, -1.5476]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5484, -1.8529, -1.4693, -1.6762, -1.5443]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5501, -1.8869, -1.4693, -1.6820, -1.5135]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5997, -1.7626, -1.4762, -1.5487, -1.6852]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6079, -1.7704, -1.4550, -1.5511, -1.6928]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5484, -1.8948, -1.4722, -1.6455, -1.5383]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5762, -1.9582, -1.4770, -1.5628, -1.5405]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5286, -1.9213, -1.5273, -1.5275, -1.5968]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5526, -1.8464, -1.4841, -1.6260, -1.5747]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5743, -1.8566, -1.4813, -1.6546, -1.5226]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6296, -1.7484, -1.4763, -1.5349, -1.6820]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5949, -1.9394, -1.4899, -1.5564, -1.5279]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.4896, -1.9025, -1.5943, -1.5256, -1.5857]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5755, -1.9041, -1.4853, -1.5384, -1.5946]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6210, -1.7329, -1.4775, -1.5404, -1.6982]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5077, -1.9590, -1.5521, -1.5669, -1.5275]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5124, -1.9615, -1.5581, -1.5740, -1.5086]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5637, -1.8077, -1.4999, -1.6671, -1.5388]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5724, -1.8184, -1.5011, -1.6743, -1.5149]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5752, -1.7647, -1.5075, -1.6752, -1.5461]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6443, -1.6725, -1.4896, -1.5695, -1.6849]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5979, -1.7642, -1.5086, -1.6938, -1.5082]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5887, -1.7317, -1.5188, -1.6875, -1.5376]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6068, -1.7726, -1.5000, -1.7023, -1.4953]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5942, -1.7404, -1.5086, -1.6940, -1.5303]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6080, -1.7454, -1.5100, -1.6960, -1.5104]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5204, -1.9234, -1.5739, -1.5882, -1.4973]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5485, -1.8499, -1.5592, -1.5729, -1.5491]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.4910, -1.9327, -1.5827, -1.5925, -1.5084]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5484, -1.8609, -1.5107, -1.5746, -1.5896]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6168, -1.6753, -1.5085, -1.5840, -1.6722]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6235, -1.6821, -1.5099, -1.5697, -1.6725]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5533, -1.9209, -1.5202, -1.5959, -1.5115]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5548, -1.7964, -1.5163, -1.6813, -1.5269]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5681, -1.7710, -1.5123, -1.6789, -1.5398]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5451, -1.8654, -1.5056, -1.5610, -1.6091]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5321, -1.8114, -1.5334, -1.5534, -1.6442]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5698, -1.7435, -1.5188, -1.6826, -1.5505]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5350, -1.8579, -1.5153, -1.5575, -1.6188]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5249, -1.8476, -1.5583, -1.5643, -1.5847]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5944, -1.7845, -1.5000, -1.6906, -1.5071]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5348, -1.8374, -1.5430, -1.5709, -1.5913]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6073, -1.7666, -1.4936, -1.6931, -1.5134]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6455, -1.6614, -1.4770, -1.5784, -1.7005]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6018, -1.7228, -1.4987, -1.6866, -1.5543]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6000, -1.7243, -1.4872, -1.6949, -1.5598]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5768, -1.8792, -1.4954, -1.6018, -1.5374]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5477, -1.8201, -1.5266, -1.5788, -1.6008]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5610, -1.8225, -1.5291, -1.5728, -1.5883]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6699, -1.6387, -1.4824, -1.5783, -1.6924]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6241, -1.7322, -1.5110, -1.7114, -1.4929]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5962, -1.7373, -1.5077, -1.7057, -1.5220]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5856, -1.8832, -1.5106, -1.6052, -1.5076]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5933, -1.7484, -1.5090, -1.7158, -1.5063]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6160, -1.7523, -1.5151, -1.7207, -1.4731]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5475, -1.8281, -1.5587, -1.5783, -1.5618]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5974, -1.7266, -1.5158, -1.6860, -1.5381]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5613, -1.7421, -1.5240, -1.6953, -1.5437]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5488, -1.8228, -1.5465, -1.5853, -1.5700]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6178, -1.7411, -1.5179, -1.7059, -1.4892]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5766, -1.7284, -1.5150, -1.7085, -1.5380]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.4837, -1.9109, -1.5793, -1.6052, -1.5220]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5534, -1.7691, -1.5325, -1.5736, -1.6364]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5449, -1.7274, -1.5287, -1.7154, -1.5502]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5294, -1.7332, -1.5259, -1.7083, -1.5701]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5342, -1.8133, -1.5524, -1.5802, -1.5917]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5379, -1.8171, -1.5588, -1.5848, -1.5736]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5892, -1.6967, -1.5210, -1.7129, -1.5429]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6538, -1.6165, -1.4956, -1.5994, -1.6930]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5436, -1.7324, -1.5115, -1.7336, -1.5498]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5596, -1.7355, -1.4876, -1.7405, -1.5506]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5408, -1.7448, -1.4942, -1.7445, -1.5514]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5321, -1.8209, -1.5494, -1.6011, -1.5702]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5782, -1.7089, -1.4884, -1.7254, -1.5666]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5895, -1.7395, -1.5039, -1.7248, -1.5147]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5792, -1.7418, -1.5180, -1.7343, -1.5007]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5303, -1.8275, -1.5546, -1.6022, -1.5607]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.4559, -1.9270, -1.5792, -1.6315, -1.5167]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5862, -1.7194, -1.5126, -1.7112, -1.5364]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5538, -1.8338, -1.5558, -1.5974, -1.5356]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5522, -1.8351, -1.5005, -1.6067, -1.5844]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.4686, -1.8540, -1.5977, -1.5850, -1.5807]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5272, -1.7762, -1.5056, -1.7409, -1.5310]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5345, -1.9200, -1.5194, -1.6368, -1.4944]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.4401, -1.9480, -1.5889, -1.6456, -1.4984]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.4464, -1.9513, -1.5941, -1.6508, -1.4806]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5206, -1.7880, -1.5217, -1.7558, -1.5005]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5275, -1.8547, -1.5543, -1.6224, -1.5248]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5748, -1.7302, -1.5114, -1.7192, -1.5329]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.4634, -1.9335, -1.5936, -1.6617, -1.4657]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5238, -1.7656, -1.5124, -1.7677, -1.5145]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6424, -1.6208, -1.4962, -1.6268, -1.6702]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.4566, -1.9279, -1.5863, -1.6650, -1.4800]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.4575, -1.9287, -1.5916, -1.6770, -1.4641]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6520, -1.5980, -1.5080, -1.6426, -1.6544]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5441, -1.7492, -1.5074, -1.7840, -1.5001]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5518, -1.8999, -1.5326, -1.6806, -1.4425]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.4705, -1.8310, -1.6102, -1.6283, -1.5425]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5878, -1.7396, -1.5318, -1.7748, -1.4508]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.4529, -1.9344, -1.6125, -1.6964, -1.4318]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6384, -1.5913, -1.5267, -1.6440, -1.6524]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5301, -1.8105, -1.5761, -1.6563, -1.5038]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5638, -1.9008, -1.5549, -1.6879, -1.4061]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.4550, -1.9329, -1.6346, -1.6737, -1.4301]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.4370, -1.9401, -1.6410, -1.6795, -1.4338]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6434, -1.5843, -1.5370, -1.6411, -1.6460]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.4395, -1.8290, -1.6442, -1.6198, -1.5546]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5181, -1.8179, -1.5859, -1.6330, -1.5214]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5130, -1.8148, -1.5754, -1.6449, -1.5282]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.4928, -1.7704, -1.5415, -1.7933, -1.4938]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5091, -1.9154, -1.5681, -1.6866, -1.4355]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.4008, -1.8501, -1.6435, -1.6244, -1.5796]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5309, -1.7125, -1.5478, -1.7590, -1.5219]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.4910, -1.9271, -1.5825, -1.6621, -1.4521]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.4661, -1.7835, -1.5591, -1.7602, -1.5196]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.3867, -1.9681, -1.6579, -1.6552, -1.4749]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.4635, -1.8468, -1.6062, -1.6197, -1.5501]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.4671, -1.7993, -1.5504, -1.7618, -1.5137]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5519, -1.7651, -1.5450, -1.7553, -1.4661]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.4916, -1.8566, -1.5711, -1.6253, -1.5412]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.4873, -1.8574, -1.5679, -1.6392, -1.5356]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.4891, -1.8632, -1.5783, -1.6325, -1.5256]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.4010, -1.9816, -1.6374, -1.6811, -1.4476]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6202, -1.6063, -1.5356, -1.6432, -1.6460]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5635, -1.7650, -1.5503, -1.7731, -1.4379]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.4813, -1.8553, -1.5806, -1.6351, -1.5349]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.4134, -1.9817, -1.6418, -1.6785, -1.4331]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5285, -1.7446, -1.5393, -1.7433, -1.5189]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5520, -1.7689, -1.5629, -1.7715, -1.4352]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.4957, -1.8453, -1.5599, -1.6542, -1.5300]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5048, -1.8498, -1.5522, -1.6523, -1.5265]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6209, -1.5847, -1.5362, -1.6432, -1.6675]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.4956, -1.8598, -1.5682, -1.6451, -1.5197]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5194, -1.8649, -1.5407, -1.6453, -1.5184]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5045, -1.8675, -1.5382, -1.6533, -1.5271]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.4331, -1.8914, -1.5927, -1.6464, -1.5396]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5029, -1.7930, -1.5299, -1.6627, -1.5854]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6293, -1.5826, -1.5306, -1.6372, -1.6737]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5684, -1.7609, -1.5421, -1.7756, -1.4421]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5643, -1.7325, -1.5254, -1.7740, -1.4840]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5137, -1.8488, -1.5426, -1.6707, -1.5117]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6441, -1.5725, -1.5264, -1.6692, -1.6421]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6650, -1.5602, -1.5238, -1.6635, -1.6433]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5813, -1.6892, -1.5333, -1.7692, -1.4991]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5335, -1.8205, -1.5295, -1.6712, -1.5251]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5458, -1.8322, -1.5211, -1.6834, -1.5024]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6663, -1.5395, -1.5057, -1.6876, -1.6622]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6877, -1.5272, -1.5032, -1.6821, -1.6633]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.4561, -1.8315, -1.5755, -1.6803, -1.5443]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6021, -1.6809, -1.4980, -1.7973, -1.5007]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5335, -1.8141, -1.5039, -1.6906, -1.5392]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5529, -1.8204, -1.5067, -1.6794, -1.5220]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.4250, -1.9270, -1.6137, -1.7149, -1.4491]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.4316, -1.9306, -1.6191, -1.7201, -1.4318]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5982, -1.7028, -1.5329, -1.8096, -1.4441]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5980, -1.6758, -1.5008, -1.7888, -1.5123]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5544, -1.8047, -1.5056, -1.7010, -1.5151]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5018, -1.7587, -1.5075, -1.8193, -1.5078]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5186, -1.7615, -1.4893, -1.8389, -1.4931]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5325, -1.8201, -1.5037, -1.7030, -1.5254]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5264, -1.8021, -1.5079, -1.6996, -1.5437]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5412, -1.7997, -1.5005, -1.7069, -1.5321]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5957, -1.6865, -1.4759, -1.8094, -1.5152]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5456, -1.8109, -1.4901, -1.6913, -1.5433]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.4454, -1.8316, -1.5577, -1.6696, -1.5835]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5213, -1.9119, -1.5147, -1.7001, -1.4644]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6022, -1.6954, -1.4641, -1.7849, -1.5328]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6196, -1.7068, -1.4938, -1.8032, -1.4641]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.4914, -1.7631, -1.5071, -1.8018, -1.5284]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5859, -1.6858, -1.4807, -1.7796, -1.5430]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5380, -1.8109, -1.5021, -1.6615, -1.5647]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5413, -1.8185, -1.5044, -1.6662, -1.5488]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5320, -1.8158, -1.5169, -1.6567, -1.5558]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6786, -1.5272, -1.4910, -1.6686, -1.7007]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5860, -1.7146, -1.5150, -1.7893, -1.4769]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5315, -1.9096, -1.5298, -1.6935, -1.4472]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5105, -1.7306, -1.5259, -1.6607, -1.6369]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6835, -1.5010, -1.5116, -1.6848, -1.6857]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6784, -1.5072, -1.4991, -1.6868, -1.6963]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.4166, -1.9449, -1.6126, -1.7016, -1.4580]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6023, -1.6898, -1.4738, -1.7760, -1.5342]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6102, -1.6902, -1.4828, -1.7801, -1.5138]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.4190, -1.9582, -1.6149, -1.6989, -1.4476]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5196, -1.9214, -1.5441, -1.6918, -1.4392]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6764, -1.5192, -1.5093, -1.6528, -1.7065]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6019, -1.6899, -1.4984, -1.7719, -1.5121]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.3969, -1.9578, -1.6270, -1.7121, -1.4504]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.4239, -1.9529, -1.6170, -1.7200, -1.4278]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5970, -1.6951, -1.4952, -1.7806, -1.5088]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5454, -1.8221, -1.5232, -1.6676, -1.5215]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6396, -1.7302, -1.4897, -1.7977, -1.4373]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6353, -1.7045, -1.4603, -1.7887, -1.4965]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6395, -1.7081, -1.4624, -1.7700, -1.5020]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6412, -1.7155, -1.4649, -1.7490, -1.5081]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5429, -1.9284, -1.5509, -1.6929, -1.4072]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6440, -1.7177, -1.5132, -1.7513, -1.4543]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5493, -1.8092, -1.5350, -1.6406, -1.5395]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6547, -1.7063, -1.5129, -1.7689, -1.4417]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6599, -1.6853, -1.5199, -1.7709, -1.4459]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6461, -1.6605, -1.4889, -1.7439, -1.5293]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5664, -1.9101, -1.5576, -1.6563, -1.4203]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5450, -1.7575, -1.5213, -1.7623, -1.4954]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.7178, -1.4887, -1.5199, -1.6477, -1.6948]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5558, -1.8017, -1.5370, -1.6281, -1.5483]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5798, -1.8003, -1.5344, -1.6072, -1.5478]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5807, -1.8000, -1.5392, -1.6207, -1.5298]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5479, -1.7577, -1.5167, -1.7618, -1.4974]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.4347, -1.8399, -1.6053, -1.6214, -1.5871]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5659, -1.9203, -1.5544, -1.6658, -1.4098]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5558, -1.8015, -1.5314, -1.6421, -1.5413]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6409, -1.6821, -1.4914, -1.7137, -1.5373]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.4145, -1.8522, -1.6170, -1.6061, -1.6049]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5253, -1.8361, -1.5539, -1.6081, -1.5545]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5159, -1.8401, -1.5511, -1.6087, -1.5634]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5049, -1.8333, -1.5480, -1.6225, -1.5702]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6940, -1.4910, -1.5231, -1.6243, -1.7374]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6286, -1.6880, -1.4911, -1.6959, -1.5590]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5278, -1.8212, -1.5316, -1.6186, -1.5760]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.4101, -1.9758, -1.6287, -1.6671, -1.4599]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5401, -1.8324, -1.5133, -1.6179, -1.5745]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6622, -1.6944, -1.5077, -1.7363, -1.4741]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5121, -1.7755, -1.5192, -1.7624, -1.5158]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5085, -1.7193, -1.5359, -1.6355, -1.6636]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6427, -1.6913, -1.5223, -1.7280, -1.4854]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5038, -1.7512, -1.5289, -1.7715, -1.5265]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5073, -1.7927, -1.5471, -1.6374, -1.5865]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5292, -1.7892, -1.5190, -1.6289, -1.6038]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5158, -1.6959, -1.5355, -1.6347, -1.6790]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5300, -1.7871, -1.5328, -1.6251, -1.5936]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5568, -1.7881, -1.5235, -1.6054, -1.5938]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.4234, -1.9418, -1.6442, -1.6810, -1.4426]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.4186, -1.9458, -1.6423, -1.6836, -1.4446]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.3749, -1.9638, -1.6654, -1.6858, -1.4593]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6334, -1.6696, -1.4897, -1.7148, -1.5561]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5513, -1.7997, -1.5422, -1.6013, -1.5745]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.7033, -1.4757, -1.5469, -1.6188, -1.7245]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6455, -1.6611, -1.5039, -1.6836, -1.5645]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5395, -1.6761, -1.5550, -1.6263, -1.6577]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5495, -1.7583, -1.5580, -1.5853, -1.6101]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5557, -1.8745, -1.5855, -1.6493, -1.4334]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5472, -1.6366, -1.5679, -1.6348, -1.6658]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6751, -1.6213, -1.5535, -1.7283, -1.4873]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.7361, -1.4168, -1.5631, -1.6029, -1.7689]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5669, -1.7178, -1.5707, -1.5759, -1.6239]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.7448, -1.3967, -1.5664, -1.6283, -1.7553]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5568, -1.5987, -1.5701, -1.6244, -1.7039]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5732, -1.5788, -1.5721, -1.6443, -1.6839]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.7826, -1.3621, -1.5662, -1.6331, -1.7631]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5693, -1.6816, -1.5761, -1.6057, -1.6185]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5541, -1.6793, -1.5801, -1.6286, -1.6098]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5868, -1.6821, -1.5585, -1.6048, -1.6192]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6865, -1.5634, -1.5194, -1.6831, -1.6056]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5955, -1.6900, -1.5651, -1.5900, -1.6110]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5731, -1.6773, -1.5803, -1.5990, -1.6209]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6042, -1.6745, -1.5751, -1.5777, -1.6189]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.4726, -1.7090, -1.6468, -1.5771, -1.6588]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6857, -1.5792, -1.5728, -1.6923, -1.5279]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.7026, -1.5596, -1.5759, -1.7151, -1.5105]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5606, -1.6327, -1.5622, -1.7378, -1.5654]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.7916, -1.3271, -1.5777, -1.6422, -1.7835]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5641, -1.6179, -1.5528, -1.7547, -1.5712]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.4157, -1.8538, -1.6862, -1.6648, -1.4866]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5430, -1.8019, -1.5983, -1.6297, -1.5003]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6735, -1.5536, -1.5858, -1.7114, -1.5347]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5883, -1.6702, -1.5852, -1.5702, -1.6367]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.7757, -1.3243, -1.5933, -1.6309, -1.7979]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.7784, -1.3309, -1.5977, -1.6122, -1.8015]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6891, -1.5516, -1.5500, -1.6641, -1.6006]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6957, -1.5560, -1.5330, -1.6678, -1.6044]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5751, -1.6919, -1.5867, -1.5645, -1.6345]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5784, -1.8345, -1.5938, -1.6138, -1.4623]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6087, -1.6837, -1.5799, -1.5658, -1.6132]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.8115, -1.3519, -1.5849, -1.5984, -1.7681]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6987, -1.5873, -1.5799, -1.6884, -1.5060]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5498, -1.6772, -1.5655, -1.7055, -1.5599]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5954, -1.8465, -1.5982, -1.5814, -1.4635]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6164, -1.7001, -1.5860, -1.5446, -1.6065]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5757, -1.5795, -1.5969, -1.5789, -1.7242]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5887, -1.7010, -1.5826, -1.5740, -1.6063]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5920, -1.8476, -1.5963, -1.5719, -1.4760]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.8014, -1.3579, -1.5863, -1.5581, -1.8170]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5854, -1.7154, -1.5794, -1.5600, -1.6144]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6837, -1.5857, -1.5300, -1.6305, -1.6240]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5931, -1.5746, -1.5784, -1.5699, -1.7416]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.4362, -1.8961, -1.6684, -1.5977, -1.5091]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.7191, -1.5804, -1.5037, -1.6326, -1.6238]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.8176, -1.3472, -1.5767, -1.5729, -1.8109]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.7238, -1.5685, -1.4938, -1.6434, -1.6326]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6028, -1.6918, -1.5457, -1.5760, -1.6373]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.7308, -1.5557, -1.5370, -1.6668, -1.5705]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5738, -1.6989, -1.5515, -1.5768, -1.6539]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6979, -1.5617, -1.5105, -1.6389, -1.6494]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6184, -1.6812, -1.5697, -1.5365, -1.6481]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.7995, -1.3529, -1.5712, -1.5667, -1.8351]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5928, -1.6771, -1.5797, -1.5265, -1.6799]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6096, -1.6669, -1.5779, -1.5392, -1.6595]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5881, -1.6799, -1.5516, -1.5870, -1.6458]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.8152, -1.3349, -1.5656, -1.5649, -1.8584]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5889, -1.8315, -1.5662, -1.5806, -1.5097]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5633, -1.6482, -1.5327, -1.7087, -1.6039]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.4117, -1.8967, -1.6608, -1.6200, -1.5218]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.8058, -1.3576, -1.5525, -1.5818, -1.8260]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.8043, -1.3474, -1.5410, -1.5978, -1.8390]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6824, -1.5507, -1.5111, -1.6544, -1.6602]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.7217, -1.5613, -1.5251, -1.6644, -1.5874]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5982, -1.6730, -1.5630, -1.5455, -1.6748]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.5528, -1.8449, -1.5800, -1.5833, -1.5186]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6174, -1.6565, -1.5672, -1.5577, -1.6528]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6331, -1.6643, -1.5696, -1.5492, -1.6357]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.8467, -1.3331, -1.5367, -1.5843, -1.8426]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.7444, -1.5403, -1.4942, -1.6312, -1.6565]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.8504, -1.3330, -1.5472, -1.5793, -1.8315]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.7316, -1.5405, -1.5098, -1.6163, -1.6654]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.7596, -1.5258, -1.5116, -1.6271, -1.6431]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.4653, -1.6824, -1.6645, -1.5609, -1.6937]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.7512, -1.5348, -1.5167, -1.6123, -1.6500]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.4165, -1.8793, -1.6753, -1.5978, -1.5364]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.8683, -1.3272, -1.5636, -1.5503, -1.8399]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.4592, -1.8686, -1.6621, -1.5970, -1.5097]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.8334, -1.3450, -1.5835, -1.5354, -1.8390]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.8691, -1.3377, -1.5761, -1.5281, -1.8353]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6076, -1.5331, -1.6095, -1.5681, -1.7411]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6054, -1.6801, -1.5679, -1.5344, -1.6672]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.4376, -1.8892, -1.6794, -1.5885, -1.5114]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.6545, -1.6452, -1.5996, -1.5246, -1.6289]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.7826, -1.5560, -1.5708, -1.6150, -1.5415]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.7422, -1.5414, -1.5525, -1.6065, -1.6172]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Output: tensor([[-1.4465, -1.9008, -1.6944, -1.6023, -1.4696]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "# n_iters = 100000\n",
    "epochs = 200\n",
    "print_every = 5000\n",
    "plot_every = 1000\n",
    "\n",
    "\n",
    "\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# train_ds = torch.utils.data.TensorDataset(data_x_torch, data_y_torch)\n",
    "# train_dl = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "    \n",
    "#     output, loss = train(category_tensor, line_tensor)\n",
    "#     current_loss += loss\n",
    "\n",
    "\n",
    "# Let's try with 1 epoch\n",
    "for category, line_tensor in tqdm(zip(data_y, data_x_torch)):\n",
    "    output, loss = train(torch.tensor([category]), line_tensor)\n",
    "\n",
    "    all_losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42d5518c-d08d-4dc6-b41b-93b39742d654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x20d132446a0>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the loss\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172cc91d-3d94-49d5-9387-9e3f6f8025b5",
   "metadata": {},
   "source": [
    "# Prediction and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03833523-d7f2-49b4-be57-6ba76002c9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE SURE THAT YOU HAVE THE RIGHT FORMAT\n",
    "assert prediction.ndim == 1\n",
    "assert prediction.shape[0] == 250\n",
    "\n",
    "# AND SAVE EXACTLY AS SHOWN BELOW\n",
    "np.save('prediction.npy', prediction.astype(int))\n",
    "\n",
    "# MAKE SURE THAT THE FILE HAS THE CORRECT FORMAT\n",
    "def validate_prediction_format():\n",
    "    loaded = np.load('prediction.npy')\n",
    "    assert loaded.shape == (250, )\n",
    "    assert loaded.dtype == int\n",
    "    assert (loaded <= 4).all()\n",
    "    assert (loaded >= 0).all()\n",
    "validate_prediction_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20e2910-300b-439f-bb5d-65752dfcf1bc",
   "metadata": {},
   "source": [
    "**accuracy** → **points**<br>\n",
    "≥95% → 10<br>\n",
    "≥90% → 7<br>\n",
    "≥85% → 5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLenv",
   "language": "python",
   "name": "dlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
